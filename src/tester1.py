

import spacy
nlp = spacy.load('en_core_web_lg')
text = "hi there today we'll look at group normalization by Yu Xin Wu and coming her of Facebook AI research so this paper is basically an engineering paper about a new normalization technique called group normalization so what's the issue here the issue is that pretty much throughout neural network learning we're using this technique called batch normalization now batch normalization is a pretty reasonable thing and it works very very well so what's the idea behind batch normalization the idea is if you have data points for machine learning methods and your data is in a 2d coordinate system somewhere down here right and you're trying to separate that from the dots which are here it is often very beneficial to shift that distribution before you do anything you want to shift it to the middle of the basically want to Center it first of all such that the 0 the origin point is in the middle of the data and then sometimes you also want to do what's called normalize it and by normalizing we mean you want to kind of rescale the axis such that things are more or less sort of like gaussians so if you if you look at this distribution first is the centering and then second is what is called a normalization normalization and usually we know that any sort of machine learning methods work better if you do that and that's mostly in classic machine learning methods with conditioning numbers of the data being better and so on but if you just want to learn let's say a linear classifier you can see here you can even save one parameter because you can make it just go through the origin and that's true in general so if we draw this in 1d you'd have a distribution that maybe is very peaky right here you first center it to the middle of the coordinate system and sorry that's not really centered and then you would divide it by its standard deviation such that after it it is a unit standard deviation Gaussian so a normal distribution the closer your data seems to be to a multivariate normal distribution the better these machine learning methods work especially if you look at how signal in deep network is propagating through the layers so the idea is if it's good for the general machine learning method that the input has a multivariate normal distribution or is normalized then it's probably good that the input to each layer is normalized so when you look at how signal features are in between layers so this is for example the 5/3 this is a layer somewhere in the middle of a convolutional neural network and if you look at the spread of how features are feature signals are throughout training you'll see that the more training progresses the larger the kind of spread of features is so you might get really large numbers or really large negative numbers or maybe really small numbers in "
text_sentences = nlp(text)
for idx, sentence in enumerate(text_sentences.sents):
    print("{}. {}".format(idx, sentence.text))


